{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bA5yPeNHGiF3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import DataFrame as df\n",
        "import math, sys, os\n",
        "import random as rand\n",
        "from itertools import combinations, permutations\n",
        "import pickle as pkl\n",
        "import threading\n",
        "from sklearn.metrics import silhouette_score,davies_bouldin_score,calinski_harabasz_score\n",
        "from sklearn.cluster import KMeans, Birch\n",
        "from sklearn import metrics\n",
        "import copy\n",
        "from numpy import random\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.cm as cm\n",
        "from multiprocessing import Process\n",
        "from tabulate import tabulate\n",
        "import warnings\n",
        "import statistics\n",
        "import warnings\n",
        "window_size=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-kG3JqEoGOJz"
      },
      "outputs": [],
      "source": [
        "class RCAC:\n",
        "  def __init__(self,Dataset_name,split_size,num_clusters,data_drop_columns, compare_with_others = False, trials = 400,num_threads = 1000) -> None:\n",
        "    ## Make changes only in this section. Preferably don't change path\n",
        "    ## The code will automatically make the directories if they don't exist.\n",
        "\n",
        "    self.save_location = '.temp/'\n",
        "    self.trials = trials\n",
        "    self.num_threads = num_threads\n",
        "    self.labels_ = []\n",
        "    self.best_so_far = 0\n",
        "    self.Dataset_name = Dataset_name\n",
        "    self.rule_kind = 'best'\n",
        "    self.split_size = split_size\n",
        "    self.num_clusters = num_clusters\n",
        "    self.split_index = 0\n",
        "    self.compare = compare_with_others\n",
        "    self.data_drop_columns = data_drop_columns\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    rule_list_name = self.rule_kind+'_cycles_'+str(self.split_size)\n",
        "    os.makedirs('./'+self.save_location+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/Final Clusters', exist_ok=True)\n",
        "    os.makedirs('./config/'+self.Dataset_name+'/Custers-'+str(self.num_clusters), exist_ok=True)\n",
        "\n",
        "    self.data=pd.read_csv(\"./data/\"+self.Dataset_name+\".csv\")\n",
        "    self.data=self.data.dropna()\n",
        "    self.data=self.data.drop(self.data_drop_columns,axis=1)\n",
        "    window_size = 5\n",
        "    #encoder = BiNCE(self.data, length_of_each=2)\n",
        "    #self.enc = encoder.encode()\n",
        "    # if you want to apply freq based encoding,\n",
        "    self.enc = freq_encoding(self.data)\n",
        "    #self.enc = list(self.enc[self.enc.columns[0]])\n",
        "    #print(self.enc)\n",
        "\n",
        "    split_enc, num_of_splits = split_string(self.enc,self.split_size)\n",
        "    rule_list = self.get_rule_list('./rules/'+rule_list_name+'.txt')\n",
        "    rules_comb = list(combinations(rule_list, 2))\n",
        "\n",
        "\n",
        "    self.cellular_automata_clustering(rule_list,num_of_splits,rules_comb , split_enc, window_size)\n",
        "\n",
        "\n",
        "  def get_rule_list(self,path=\"./rule_list.txt\"):\n",
        "      # check if best rules exist\n",
        "      best_path = './config/'+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/'+self.rule_kind+'_rules.txt'\n",
        "      try:\n",
        "        print(\"Found best configuration!\")\n",
        "        my_file = open(best_path, 'r')\n",
        "        file_con = my_file.read()\n",
        "        # replacing end splitting the text\n",
        "\n",
        "        rule_list = file_con.split(\"\\n\")\n",
        "        my_file.close()\n",
        "        self.num_threads = 1\n",
        "        self.trials = 2\n",
        "        return rule_list\n",
        "      except:\n",
        "        print(\"No best rules yet. Running trials...\")\n",
        "      # opening the file in read mode\n",
        "      my_file = open(path, \"r\")\n",
        "      file_con = my_file.read()\n",
        "      rule_list = file_con.split(\"\\n\")\n",
        "      my_file.close()\n",
        "      return rule_list\n",
        "  def aggregate_scores(self):\n",
        "    path = './'+self.save_location+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/'\n",
        "    main_df = pd.DataFrame()\n",
        "    for files in os.listdir(path):\n",
        "        if '.DS_Store' in files or '.csv' not in files or 'final_scores' in files:\n",
        "            continue\n",
        "        df = pd.read_csv(path+files,index_col=False)\n",
        "        main_df = main_df.append(df)\n",
        "\n",
        "    main_df.columns = [\"Index\",\"Rule 1\", \"Rule 2\", \"Rule 3\", \"CA Silhoutte\",\"Heir Silhoutte\", \"Kmeans Silhoutte\", \"Birch Silhoutte\"]\n",
        "    main_df = main_df.sort_values('CA Silhoutte', ascending=False)\n",
        "    main_df.reset_index(inplace = True, drop=True)\n",
        "    print(main_df)\n",
        "\n",
        "    best_score = main_df['CA Silhoutte'].iloc[0]\n",
        "    best_rules = [main_df['Rule 1'].iloc[0], main_df['Rule 2'].iloc[0]]\n",
        "    main_df.to_csv('./'+self.save_location+'/'+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/'+self.rule_kind+'_final_scores.csv')\n",
        "    return best_score, best_rules\n",
        "\n",
        "\n",
        "#**********************Our Proposed Algorithm Starts Here***************************\n",
        "\n",
        "  def cellular_automata_clustering(self,rule_list, split, rules_comb, encoding, window_size = 5):\n",
        "    better_score_list = []\n",
        "    best_CA_sill = -10000\n",
        "    best_rule = []\n",
        "    output_data = []\n",
        "    columns = [\"Rule 1\", \"Rule 2\", \"Rule 3\", \"CA Silhoutte\",\"Heir Silhoutte\", \"Kmeans Silhoutte\", \"Birch Silhoutte\"]\n",
        "\n",
        "\n",
        "\n",
        "    rule_list_name = self.rule_kind+'_cycles_'+str(self.split_size)\n",
        "\n",
        "    rule_list3=copy.deepcopy(rule_list)\n",
        "    print(\"Rule3list\",rule_list3)\n",
        "\n",
        "    for rule_set in rules_comb:\n",
        "      for rule3 in rule_list3:\n",
        "        enc1 = copy.deepcopy(encoding)\n",
        "#***********************************************Stage1***************************************************************\n",
        "        for p in range(int(len(rule_set)/2)):\n",
        "          rule = rg(int(rule_set[0]), window_size)\n",
        "          fc = Stage1(rule,enc1,split)\n",
        "\n",
        "#***********************************************Stage2***************************************************************\n",
        "          rule=rg(int(rule_set[1]),window_size)\n",
        "\n",
        "          R3=rg(int(rule3),window_size)\n",
        "          stage2_output=[]\n",
        "          stage2_output=Stage2(fc,enc1,rule,R3)\n",
        "          stage2_dataset=stage2_output[0]\n",
        "          stage2_cycles=stage2_output[1]\n",
        "\n",
        "#******************************************************Stage3************************************************************************************\n",
        "          enc_data_=Stage3(stage2_dataset,stage2_cycles,self.num_clusters)\n",
        "          X=self.data.to_numpy()\n",
        "          try :\n",
        "            CA_sill_new=silhouette_score(X,enc_data_,metric=\"euclidean\")\n",
        "            print(\"davies:CA\",davies_bouldin_score(X,enc_data_))\n",
        "            print(\"calinski:CA\",calinski_harabasz_score(X,enc_data_))\n",
        "          except:\n",
        "            CA_sill_new = 0\n",
        "\n",
        "\n",
        "\n",
        "          #### ADD OTHER CLUSTERING METHODS FOR COMPARISON HERE ########\n",
        "\n",
        "          ## Heirarchical\n",
        "          clusterer = AgglomerativeClustering(n_clusters=self.num_clusters, affinity='euclidean', linkage='ward')\n",
        "          cluster_labels = clusterer.fit_predict(X)\n",
        "          try:\n",
        "            Heir_sill_new = silhouette_score(X, cluster_labels)\n",
        "            print(\"davies:Heirarchical\",davies_bouldin_score(X,cluster_labels))\n",
        "            print(\"calinski:Heirarchical\",calinski_harabasz_score(X,cluster_labels))\n",
        "          except:\n",
        "            Heir_sill_new = 0\n",
        "\n",
        "\n",
        "          ## Kmeans\n",
        "          clust=KMeans(n_clusters=self.num_clusters,random_state=42)\n",
        "          clust.fit(X)\n",
        "          Km_labels=clust.labels_\n",
        "          print(Km_labels)\n",
        "          try:\n",
        "            Kmeans_sill_new=metrics.silhouette_score(X, Km_labels, metric='euclidean')\n",
        "            print(\"davies:Kmeans\",davies_bouldin_score(X,Km_labels))\n",
        "            print(\"calinski:Kmeans\",calinski_harabasz_score(X,Km_labels))\n",
        "          except:\n",
        "            Kmeans_sill_new = 0\n",
        "\n",
        "\n",
        "\n",
        "          ## Birch\n",
        "          clust_model = Birch(n_clusters=self.num_clusters,branching_factor=1500,threshold=1.5)\n",
        "          clust_model.fit(X)\n",
        "          labels = clust_model.labels_\n",
        "          try:\n",
        "            Birch_new=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "            print(\"davies:Birch\",davies_bouldin_score(X,labels))\n",
        "            print(\"calinski:Birch\",calinski_harabasz_score(X,labels))\n",
        "          except Exception as e:\n",
        "\n",
        "            Birch_new = 0\n",
        "\n",
        "\n",
        "\n",
        "          # Add rto output data\n",
        "          output_data.append([rule_set[0],rule_set[1],rule3,CA_sill_new,Heir_sill_new, Kmeans_sill_new, Birch_new])\n",
        "          out_df = pd.DataFrame(data=output_data, columns=columns)\n",
        "          out_df.to_csv('./'+self.save_location+'/'+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/best_'+str(self.split_index)+'_tr_'+str(1)+'.csv')\n",
        "\n",
        "          best_score, best_rules  = self.aggregate_scores()\n",
        "          if self.best_so_far < best_score:\n",
        "            self.best_so_far = best_score\n",
        "            print(\"Best silhoutte score :\", self.best_so_far)\n",
        "            self.labels_ = enc_data_\n",
        "            with open('./config/'+self.Dataset_name+'/Custers-'+str(self.num_clusters)+'/'+self.rule_kind+'_rules.txt', 'w') as file1:\n",
        "              file1.write(str(best_rules[0])+\"\\n\"+str(best_rules[1])+\"\\n\"+str(rule3))\n",
        "\n",
        "          if self.compare:\n",
        "            my_data=[[self.num_clusters,self.best_so_far,Heir_sill_new, Kmeans_sill_new, Birch_new]]\n",
        "            head=[\"Final no.of clusters\",\" Our silhoutte score\",\"Heirarchical\", \"Kmeans\", \"Birch\"]\n",
        "            print(tabulate(my_data, headers=head, tablefmt=\"grid\"))\n",
        "\n",
        "\n",
        "\n",
        "          print(\"---------------------***-----------------******----------------------***--------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3IMcVDHcy8zb"
      },
      "outputs": [],
      "source": [
        "def freq_encoding(data):\n",
        "    encoder=['00','01','11','10']\n",
        "\n",
        "    len(data)\n",
        "\n",
        "    #Frequency Based Encoding\n",
        "    details= dict()\n",
        "    for j in data.columns:\n",
        "      col=data[j]\n",
        "      sortx = dict()\n",
        "      a=list()\n",
        "      for i in col:\n",
        "        if i not in sortx:\n",
        "          sortx[i] = 1\n",
        "          a.append(i)\n",
        "        else:\n",
        "          sortx[i]+=1\n",
        "      a.sort()\n",
        "      p=0\n",
        "      s=0\n",
        "      i=0\n",
        "      l=0\n",
        "      while p<len(data):\n",
        "        m=int(sortx[a[l]])\n",
        "        if  s+m <=(math.ceil(len(data)/len(encoder))) :\n",
        "          s+=m\n",
        "          sortx[a[l]]=encoder[i]\n",
        "        else:\n",
        "          if l == 0:\n",
        "            sortx[a[l]]=encoder[i]\n",
        "            i+=1\n",
        "          else:\n",
        "            i+=1\n",
        "            sortx[a[l]]=encoder[i]\n",
        "          s=0\n",
        "        l+=1\n",
        "        p+=m\n",
        "      details[j]=sortx\n",
        "\n",
        "    #Replacing the data in table with encoding\n",
        "    data1 = df(columns = data.columns)\n",
        "    for i in range(len(data)):\n",
        "      t=[]\n",
        "      p=data.loc[i]\n",
        "      for j in range(len(p)):\n",
        "        s=details[data.columns[j]]\n",
        "        t.append(s[p[j]])\n",
        "      ar = pd.Series(t, index = data1.columns)\n",
        "      data1 = data1.append(ar,ignore_index = True)\n",
        "      #pd.concat([data1,ar])\n",
        "\n",
        "\n",
        "\n",
        "    #Generating final encoded string\n",
        "    enc=[]\n",
        "    for i in range(len(data1)):\n",
        "      p=data1.loc[i]\n",
        "      t=\"\"\n",
        "      for j in p:\n",
        "        t+=j\n",
        "      enc.append(t)\n",
        "    enc\n",
        "    return enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "A7tddUCP0bib"
      },
      "outputs": [],
      "source": [
        "def split_string(encodings, div):\n",
        "    r_ind=[]\n",
        "    enc_length = len(encodings[0])\n",
        "    # div = math.floor((enc_length)/split)+1\n",
        "    for i in range(div, enc_length, div):\n",
        "      r_ind.append(i)\n",
        "    iclust=[]\n",
        "    for i in range(len(encodings)):\n",
        "      s=0\n",
        "      for j in r_ind:\n",
        "        iclust.append(encodings[i][s:j])\n",
        "        s=j\n",
        "      iclust.append(encodings[i][s:])\n",
        "      encodings[i]=iclust\n",
        "      iclust=[]\n",
        "    return encodings, len(r_ind)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rsQosgK1z4eZ"
      },
      "outputs": [],
      "source": [
        "def rg(rule, winsize):\n",
        "    brule=bin(rule).replace(\"0b\",\"\").zfill(2**winsize)\n",
        "    brule=brule[::-1]\n",
        "    return brule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-v-PKL-pCcY5"
      },
      "outputs": [],
      "source": [
        "def Stage1(rule,enc_stage1,split):\n",
        "  fc = {}\n",
        "  tr = []\n",
        "\n",
        "  #***********************applying rule to each split********************\n",
        "\n",
        "  for i in range(split):\n",
        "    for j in range(len(enc_stage1)):\n",
        "      tr.append(enc_stage1[j][i])\n",
        "    fc[i]=apply_rule(tr,window_size,rule)\n",
        "    tr = []\n",
        "  return fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4ZU-ABf5CcQU"
      },
      "outputs": [],
      "source": [
        "def Stage2(cluster1,dataset1,R2,R3):\n",
        "\n",
        "  #***************Sorting the cluster based on median**********************\n",
        "\n",
        "  for i in range(len(cluster1)):\n",
        "    icluster_Med_sort=Median_Cycles(cluster1[i])\n",
        "    iclust=copy.deepcopy(icluster_Med_sort)\n",
        "    s=findn(len(iclust))\n",
        "\n",
        "    #*****************changing the datset elements with cluster number****************\n",
        "    for j in range(len(dataset1)):\n",
        "      plt=dataset1[j][i]\n",
        "      l=0\n",
        "      while(plt not in iclust[l]):\n",
        "        l+=1\n",
        "      dataset1[j][i]= cy_enc(s,l)\n",
        "\n",
        "\n",
        "\n",
        "  #************************************merging all split into one****************\n",
        "  for i in range(len(dataset1)):\n",
        "    iclust=\"\"\n",
        "    for j in dataset1[i]:\n",
        "      iclust+=j\n",
        "    dataset1[i]=iclust\n",
        "  enc_data=[]\n",
        "  init_clusters=[]\n",
        "\n",
        "  #If merged data length less than maximum possible cell size  apply rule 2\n",
        "  if(len(dataset1[0])<=13):\n",
        "    init_clusters=apply_rule(dataset1,window_size,R2)\n",
        "    init_cluster_Med_sort=Median_Cycles(init_clusters)\n",
        "    enc_data=copy.deepcopy(dataset1)\n",
        "    for i in range(len(dataset1)):\n",
        "      j=0\n",
        "      while j <len(init_cluster_Med_sort):\n",
        "        if dataset1[i] in init_cluster_Med_sort[j]:\n",
        "          dataset1[i]=j\n",
        "          break\n",
        "        else:\n",
        "          j+=1\n",
        "  else:\n",
        "    Stage2_output_reduced=Stage_part2(dataset1,R3)\n",
        "    return Stage2_output_reduced\n",
        "  Stage2_output=[]\n",
        "  Stage2_output.append(dataset1)\n",
        "  Stage2_output.append(init_cluster_Med_sort)\n",
        "  return Stage2_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pmyOxHvKCcHt"
      },
      "outputs": [],
      "source": [
        "def Stage_part2(dataset2,R3):\n",
        "  print(\"Rule 3 Used\")\n",
        "  split_size=13\n",
        "  split_enc2, num_of_splits2 = split_string(dataset2,split_size)\n",
        "  Stage2_part2_output1=Stage1(R3,split_enc2,num_of_splits2)\n",
        "  stage2_output2=[]\n",
        "  stage2_output2=Stage2(Stage2_part2_output1,dataset2,R3,R3)\n",
        "  return stage2_output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_wRDSzS-CzOd"
      },
      "outputs": [],
      "source": [
        "def Stage3(ST3_in,cycle_3,num_clust):\n",
        "  #Find decimal values of cluster for sorting\n",
        "  if(len(cycle_3)<num_clust):\n",
        "    return\n",
        "  Dec_CL=[]\n",
        "  for ele in cycle_3:\n",
        "    L1=[]\n",
        "    for e in ele:\n",
        "      L1.append(binaryTodecimal(int(e)))\n",
        "    Dec_CL.append(L1)\n",
        "    Median_CL=[]\n",
        "  for ele in Dec_CL:\n",
        "    Median_CL.append((statistics.median(ele)))\n",
        "  Unsorted_median=copy.deepcopy(Median_CL)\n",
        "  Median_CL.sort()\n",
        "  gap=[]\n",
        "  for i in range(len(Median_CL)-1):\n",
        "     gap.append(Median_CL[i+1]-Median_CL[i])\n",
        "  unsortgap=copy.deepcopy(gap)\n",
        "  gap.sort(reverse=True)\n",
        "  gap_index=[]\n",
        "  for i in range(num_clust-1):\n",
        "    gap_index.append(unsortgap.index(gap[i]))\n",
        "  gap_index.sort()\n",
        "  flag=[]\n",
        "  for i in range(len(gap_index)+1):\n",
        "    if(i==0):\n",
        "      for j in range(len(ST3_in)):\n",
        "        if((ST3_in[j]<=gap_index[i]) and (j not in flag)):\n",
        "          ST3_in[j]=i\n",
        "          flag.append(j)\n",
        "    elif(i>0 and i<(len(gap_index))):\n",
        "      for j in range(len(ST3_in)):\n",
        "        if((gap_index[i-1]< ST3_in[j] <= gap_index[i]) and (j not in flag)):\n",
        "          ST3_in[j]=i\n",
        "          flag.append(j)\n",
        "    elif(i==(len(gap_index))):\n",
        "      for j in range(len(ST3_in)):\n",
        "        if((gap_index[i-1]<=ST3_in[j]) and (j not in flag)):\n",
        "          ST3_in[j]=i\n",
        "          flag.append(j)\n",
        "  print(ST3_in)\n",
        "  return(ST3_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2dmWfz9-C-hg"
      },
      "outputs": [],
      "source": [
        "def apply_rule(split,winsize,brule):\n",
        "    final_array = []\n",
        "    split_list=list(set(split))\n",
        "    split_list.sort(reverse=True)\n",
        "    split_len=len(split_list[0])\n",
        "    current_array=[]\n",
        "    while(split_list):\n",
        "      curr_element=split_list[0]\n",
        "      flag=0\n",
        "      while(not flag):\n",
        "        if current_array == []:\n",
        "          current_array.append(curr_element)\n",
        "          split_list.remove(curr_element)\n",
        "        else:\n",
        "          curr_element=nullbound(split_len,winsize,curr_element)\n",
        "          t2=\"\"\n",
        "          for j in range(split_len):\n",
        "            check=int(str(curr_element[j:j+winsize]),2)\n",
        "            t2+=brule[check]\n",
        "          curr_element=t2\n",
        "          if curr_element not in current_array:\n",
        "              if curr_element in split_list:\n",
        "                split_list.remove(curr_element)\n",
        "                current_array.append(curr_element)\n",
        "          else:\n",
        "              flag=1\n",
        "      final_array.append(current_array)\n",
        "      current_array=[]\n",
        "    return final_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lWEfIU5iDGFt"
      },
      "outputs": [],
      "source": [
        "def findn(d):\n",
        "      i=1\n",
        "      while 2**i<d:\n",
        "        i+=1\n",
        "      return i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7o1NuVR2DMk9"
      },
      "outputs": [],
      "source": [
        "def cy_enc(l,index):\n",
        "      #return bin(index).replace(\"0b\",\"\").zfill(l)\n",
        "      n=index\n",
        "      gray=n ^ (n >> 1)\n",
        "      return bin(gray).replace(\"0b\",\"\").zfill(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CdmqUqBlDP1d"
      },
      "outputs": [],
      "source": [
        "def Median_Cycles(FFCC):\n",
        "  len1=len(FFCC[0][0])\n",
        "  Dec_FFCC=[]\n",
        "  for ele in FFCC:\n",
        "    L1=[]\n",
        "    for e in ele:\n",
        "      L1.append(binaryTodecimal(int(e)))\n",
        "    Dec_FFCC.append(L1)\n",
        "    Median_FFCC=[]\n",
        "  for ele in Dec_FFCC:\n",
        "    Median_FFCC.append((statistics.median(ele)))\n",
        "  for i in range(0, len(Median_FFCC)):\n",
        "    for j in range(i+1, len(Median_FFCC)):\n",
        "        if Median_FFCC[i] >= Median_FFCC[j]:\n",
        "            Median_FFCC[i], Median_FFCC[j] = Median_FFCC[j],Median_FFCC[i]\n",
        "            Dec_FFCC[i], Dec_FFCC[j] = Dec_FFCC[j],Dec_FFCC[i]\n",
        "  bin_FFCC=[]\n",
        "  for ele in Dec_FFCC:\n",
        "    L1=[]\n",
        "    for e in ele:\n",
        "      L1.append(toBinary(e, len1))\n",
        "    bin_FFCC.append(L1)\n",
        "  return(bin_FFCC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "OT8-gzI-DamE"
      },
      "outputs": [],
      "source": [
        "def nullbound(n,winsize,init_clusters_):\n",
        "    if n%2!=0:\n",
        "            if winsize%2!=0:\n",
        "              init_clusters_ = \"0\"*(winsize//2) + init_clusters_ + \"0\"*(winsize//2)\n",
        "            else:\n",
        "              init_clusters_ = \"0\"*((winsize//2)+1) + init_clusters_ + \"0\"*(winsize//2)\n",
        "    else:\n",
        "            if winsize%2!=0:\n",
        "              init_clusters_ = \"0\"*(winsize//2) + init_clusters_ + \"0\"*(winsize//2)\n",
        "            else:\n",
        "              init_clusters_ = \"0\"*((winsize//2)+1) + init_clusters_ + \"0\"*(winsize//2)\n",
        "    return init_clusters_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "P_oN8J9MDeYO"
      },
      "outputs": [],
      "source": [
        "def binaryTodecimal(n):\n",
        "    decimal = 0\n",
        "    power = 1\n",
        "    while n>0:\n",
        "        rem = n%10\n",
        "        n = n//10\n",
        "        decimal += rem*power\n",
        "        power = power*2\n",
        "    return decimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9yqjxPbBDqIH"
      },
      "outputs": [],
      "source": [
        "def toBinary(n, len):\n",
        "    binary = ''\n",
        "    i = 1 << len - 1\n",
        "    while i > 0:\n",
        "        binary += '1' if (n & i) else '0'\n",
        "        i = i // 2\n",
        "    return binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UU9jaZF1Y-",
        "outputId": "d3d720e5-6dff-47ee-a690-af4cbc2791d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "YDkZ_OJIT0cY",
        "outputId": "69eeb320-22e9-43a3-d613-596c9b7bfdea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/school_district_breakdowns_12.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-96053c051b2b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mRCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRCAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'school_district_breakdowns_12'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_drop_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jurisdiction_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_with_others\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-f73b93e030b7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./config/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Custers-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_drop_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/school_district_breakdowns_12.csv'"
          ]
        }
      ],
      "source": [
        "RCA = RCAC('school_district_breakdowns_12', 12, 2, data_drop_columns=['jurisdiction_name'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UzNweEpFyj5"
      },
      "outputs": [],
      "source": [
        "RCA = RCAC('buddymove_holiday_10',10, 2, data_drop_columns=['User Id'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sOiou3EJeY"
      },
      "outputs": [],
      "source": [
        "RCA = RCAC('IPL - IPL_11', 11, 2, data_drop_columns=['PLAYER'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5LsRivpGLCH"
      },
      "outputs": [],
      "source": [
        "RCA = RCAC('Iris_12', 12, 2, data_drop_columns=['Species'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-YL8ZBUV-2L"
      },
      "outputs": [],
      "source": [
        "RCA = RCAC('school_district_breakdowns_12', 12, 2, data_drop_columns=['jurisdiction_name'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2m1yTEqIHQZ"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('buddymove_holidayiq', 13, 2, data_drop_columns=['User Id'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71scfxKoMGqu"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('heart_failure_clinical_records_dataset', 13, 2, data_drop_columns=['DEATH_EVENT'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J8ze1_LlOyC"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('sobar-72CervicalCancerBehaviorRisk', 13, 2, data_drop_columns=['ca_cervix'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SixUjwVE6V9k"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('seeds_dataset - seeds_dataset', 13, 2, data_drop_columns=['class'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kL8F4AqBo-d"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('Wholesale customers data', 13, 2, data_drop_columns=['Channel'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK1k0gEyM3-3"
      },
      "outputs": [],
      "source": [
        "#RCA = RCAC('StoneFlakes', 13, 2, data_drop_columns=['ID'], compare_with_others = True)\n",
        "#RCA.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7jfalidC9o9"
      },
      "outputs": [],
      "source": [
        "RCA = RCAC('Iris_8',8, 2, data_drop_columns=['Species'], compare_with_others = True)\n",
        "RCA.fit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}